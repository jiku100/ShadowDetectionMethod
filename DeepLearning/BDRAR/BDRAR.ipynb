{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BDRAR",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMD5GC0OSHxamTBjG2Qv9pX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiku100/ShadowDetectionMethod/blob/main/DeepLearning/BDRAR/BDRAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eadubhCfAln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b22c4c5-85af-4bea-e7a0-098a73b0bbc7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_csnmP8Pmc5"
      },
      "source": [
        "!cp -r \"/content/gdrive/My Drive/BDRAR\" ."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzDcuFKsOrsz",
        "outputId": "d7e85ebb-f517-49e7-c82d-e9e457c8ed54"
      },
      "source": [
        "%cd BDRAR"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/BDRAR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szj96sLGQO1L"
      },
      "source": [
        "!unzip SBU-shadow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQXrUOFv5pud"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/BDRAR/resnext')\n",
        "sys.path.append('/content/BDRAR')\n",
        "print(sys.path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlonVdVoQaiW"
      },
      "source": [
        "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar59XnWLQJ0b"
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from model import BDRAR\n",
        "import joint_transforms\n",
        "from config import sbu_training_root\n",
        "from dataset import ImageFolder\n",
        "from misc import AvgMeter, check_mkdir\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "ckpt_path = './ckpt'\n",
        "exp_name = 'BDRAR'\n",
        "\n",
        "# batch size of 8 with resolution of 416*416 is exactly OK for the GTX 1080Ti GPU\n",
        "args = {\n",
        "    'iter_num': 3000,\n",
        "    'train_batch_size': 8,\n",
        "    'last_iter': 0,\n",
        "    'lr': 5e-3,\n",
        "    'lr_decay': 0.9,\n",
        "    'weight_decay': 5e-4,\n",
        "    'momentum': 0.9,\n",
        "    'snapshot': '',\n",
        "    'scale': 416\n",
        "}\n",
        "\n",
        "joint_transform = joint_transforms.Compose([\n",
        "    joint_transforms.RandomHorizontallyFlip(),\n",
        "    joint_transforms.Resize((args['scale'], args['scale']))\n",
        "])\n",
        "val_joint_transform = joint_transforms.Compose([\n",
        "    joint_transforms.Resize((args['scale'], args['scale']))\n",
        "])\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "target_transform = transforms.ToTensor()\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "train_set = ImageFolder(sbu_training_root, joint_transform, img_transform, target_transform)\n",
        "train_loader = DataLoader(train_set, batch_size=args['train_batch_size'], num_workers=8, shuffle=True)\n",
        "\n",
        "bce_logit = nn.BCEWithLogitsLoss().cuda()\n",
        "log_path = os.path.join(ckpt_path, exp_name, str(datetime.datetime.now()) + '.txt')\n",
        "\n",
        "\n",
        "def main():\n",
        "    net = BDRAR().cuda().train()\n",
        "\n",
        "    optimizer = optim.SGD([\n",
        "        {'params': [param for name, param in net.named_parameters() if name[-4:] == 'bias'],\n",
        "         'lr': 2 * args['lr']},\n",
        "        {'params': [param for name, param in net.named_parameters() if name[-4:] != 'bias'],\n",
        "         'lr': args['lr'], 'weight_decay': args['weight_decay']}\n",
        "    ], momentum=args['momentum'])\n",
        "\n",
        "    if len(args['snapshot']) > 0:\n",
        "        print('training resumes from \\'%s\\'' % args['snapshot'])\n",
        "        net.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'] + '.pth')))\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(ckpt_path, exp_name, args['snapshot'] + '_optim.pth')))\n",
        "        optimizer.param_groups[0]['lr'] = 2 * args['lr']\n",
        "        optimizer.param_groups[1]['lr'] = args['lr']\n",
        "\n",
        "    check_mkdir(ckpt_path)\n",
        "    check_mkdir(os.path.join(ckpt_path, exp_name))\n",
        "    open(log_path, 'w').write(str(args) + '\\n\\n')\n",
        "    train(net, optimizer)\n",
        "\n",
        "\n",
        "def train(net, optimizer):\n",
        "    curr_iter = args['last_iter']\n",
        "    while True:\n",
        "        train_loss_record, loss_fuse_record, loss1_h2l_record = AvgMeter(), AvgMeter(), AvgMeter()\n",
        "        loss2_h2l_record, loss3_h2l_record, loss4_h2l_record = AvgMeter(), AvgMeter(), AvgMeter()\n",
        "        loss1_l2h_record, loss2_l2h_record, loss3_l2h_record = AvgMeter(), AvgMeter(), AvgMeter()\n",
        "        loss4_l2h_record = AvgMeter()\n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            optimizer.param_groups[0]['lr'] = 2 * args['lr'] * (1 - float(curr_iter) / args['iter_num']\n",
        "                                                                ) ** args['lr_decay']\n",
        "            optimizer.param_groups[1]['lr'] = args['lr'] * (1 - float(curr_iter) / args['iter_num']\n",
        "                                                            ) ** args['lr_decay']\n",
        "\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                inputs, labels = data\n",
        "                batch_size = inputs.size(0)\n",
        "                inputs = Variable(inputs).cuda()\n",
        "                labels = Variable(labels).cuda()\n",
        "\n",
        "            \n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                fuse_predict, predict1_h2l, predict2_h2l, predict3_h2l, predict4_h2l, \\\n",
        "                predict1_l2h, predict2_l2h, predict3_l2h, predict4_l2h = net.forward(inputs)\n",
        "                loss_fuse = bce_logit(fuse_predict, labels)\n",
        "                loss1_h2l = bce_logit(predict1_h2l, labels)\n",
        "                loss2_h2l = bce_logit(predict2_h2l, labels)\n",
        "                loss3_h2l = bce_logit(predict3_h2l, labels)\n",
        "                loss4_h2l = bce_logit(predict4_h2l, labels)\n",
        "                loss1_l2h = bce_logit(predict1_l2h, labels)\n",
        "                loss2_l2h = bce_logit(predict2_l2h, labels)\n",
        "                loss3_l2h = bce_logit(predict3_l2h, labels)\n",
        "                loss4_l2h = bce_logit(predict4_l2h, labels)\n",
        "\n",
        "                loss = loss_fuse + loss1_h2l + loss2_h2l + loss3_h2l + loss4_h2l + loss1_l2h + loss2_l2h + loss3_l2h + loss4_l2h\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss_record.update(loss.data, batch_size)\n",
        "                loss_fuse_record.update(loss_fuse.data, batch_size)\n",
        "                loss1_h2l_record.update(loss1_h2l.data, batch_size)\n",
        "                loss2_h2l_record.update(loss2_h2l.data, batch_size)\n",
        "                loss3_h2l_record.update(loss3_h2l.data, batch_size)\n",
        "                loss4_h2l_record.update(loss4_h2l.data, batch_size)\n",
        "                loss1_l2h_record.update(loss1_l2h.data, batch_size)\n",
        "                loss2_l2h_record.update(loss2_l2h.data, batch_size)\n",
        "                loss3_l2h_record.update(loss3_l2h.data, batch_size)\n",
        "                loss4_l2h_record.update(loss4_l2h.data, batch_size)\n",
        "\n",
        "            curr_iter += 1\n",
        "\n",
        "            log = '[iter %d], [train loss %.5f], [loss_fuse %.5f], [loss1_h2l %.5f], [loss2_h2l %.5f], ' \\\n",
        "                  '[loss3_h2l %.5f], [loss4_h2l %.5f], [loss1_l2h %.5f], [loss2_l2h %.5f], [loss3_l2h %.5f], ' \\\n",
        "                  '[loss4_l2h %.5f], [lr %.13f]' % \\\n",
        "                  (curr_iter, train_loss_record.avg, loss_fuse_record.avg, loss1_h2l_record.avg, loss2_h2l_record.avg,\n",
        "                   loss3_h2l_record.avg, loss4_h2l_record.avg, loss1_l2h_record.avg, loss2_l2h_record.avg,\n",
        "                   loss3_l2h_record.avg, loss4_l2h_record.avg, optimizer.param_groups[1]['lr'])\n",
        "            print(log)\n",
        "            open(log_path, 'a').write(log + '\\n')\n",
        "\n",
        "            if curr_iter >= args['iter_num']:\n",
        "                date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                torch.save(net.state_dict(), os.path.join(ckpt_path, exp_name, date + '_%d.pth' % curr_iter))\n",
        "                return\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAapLwHAQKQw",
        "outputId": "f2eeadaa-90cf-4fee-aaea-7eaf93b5810f"
      },
      "source": [
        "!unzip test.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.zip\n",
            "  inflating: 1.jpg                   \n",
            "  inflating: 2.jpg                   \n",
            "  inflating: 3.jpg                   \n",
            "  inflating: 4.jpg                   \n",
            "  inflating: 5.jpg                   \n",
            "  inflating: 6.jpg                   \n",
            "  inflating: 7.jpg                   \n",
            "  inflating: Ground1.jpg             \n",
            "  inflating: Ground10.jpg            \n",
            "  inflating: Ground11.jpg            \n",
            "  inflating: Ground12.jpg            \n",
            "  inflating: Ground13.jpg            \n",
            "  inflating: Ground14.jpg            \n",
            "  inflating: Ground2.jpg             \n",
            "  inflating: Ground3.jpg             \n",
            "  inflating: Ground4.jpg             \n",
            "  inflating: Ground5.jpg             \n",
            "  inflating: Ground6.jpg             \n",
            "  inflating: Ground7.jpg             \n",
            "  inflating: Ground8.jpg             \n",
            "  inflating: Ground9.jpg             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVx5AqohnDZz",
        "outputId": "2e9fbb9b-8954-4e89-a1ff-51fa826ab10b"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "from config import sbu_testing_root\n",
        "from misc import check_mkdir, crf_refine\n",
        "from model import BDRAR\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "ckpt_path = './ckpt'\n",
        "exp_name = 'BDRAR'\n",
        "args = {\n",
        "    'snapshot': '3000',\n",
        "    'scale': 416\n",
        "}\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize(args['scale']),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "to_test = {'Ground': \"/content/BDRAR\"}\n",
        "\n",
        "to_pil = transforms.ToPILImage()\n",
        "\n",
        "loadpath = \"/content/gdrive/MyDrive/BDRAR/ckpt/BDRAR/2021-03-09 04:01:29_3000.pth\"\n",
        "if not os.path.exists(\"./Output\"):\n",
        "  os.mkdir(\"./Output\")\n",
        "\n",
        "def main():\n",
        "    net = BDRAR().cuda()\n",
        "\n",
        "    if len(args['snapshot']) > 0:\n",
        "        print('load snapshot \\'%s\\' for testing' % args['snapshot'])\n",
        "        net.load_state_dict(torch.load(loadpath))\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for name, root in to_test.items():\n",
        "            img_list = [img_name for img_name in os.listdir(root) if\n",
        "                        img_name.endswith('.jpg')]\n",
        "            for idx, img_name in enumerate(img_list):\n",
        "          \n",
        "                print('predicting for %s: %d / %d' % (name, idx + 1, len(img_list)))\n",
        "                check_mkdir(\n",
        "                    os.path.join(ckpt_path, exp_name, '(%s) %s_prediction_%s' % (exp_name, name, args['snapshot'])))\n",
        "                img = Image.open(os.path.join(root, img_name))\n",
        "                w, h = img.size\n",
        "                img_var = Variable(img_transform(img).unsqueeze(0)).cuda()\n",
        "                res = net(img_var)\n",
        "                prediction = np.array(transforms.Resize((h, w))(to_pil(res.data.squeeze(0).cpu())))\n",
        "                prediction = crf_refine(np.array(img.convert('RGB')), prediction)\n",
        "                Image.fromarray(prediction).save(\n",
        "                    os.path.join(\"/content/BDRAR/Output\", \"Output\" + str(idx) + \".jpg\"))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load snapshot '3000' for testing\n",
            "predicting for Ground: 1 / 21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predicting for Ground: 2 / 21\n",
            "predicting for Ground: 3 / 21\n",
            "predicting for Ground: 4 / 21\n",
            "predicting for Ground: 5 / 21\n",
            "predicting for Ground: 6 / 21\n",
            "predicting for Ground: 7 / 21\n",
            "predicting for Ground: 8 / 21\n",
            "predicting for Ground: 9 / 21\n",
            "predicting for Ground: 10 / 21\n",
            "predicting for Ground: 11 / 21\n",
            "predicting for Ground: 12 / 21\n",
            "predicting for Ground: 13 / 21\n",
            "predicting for Ground: 14 / 21\n",
            "predicting for Ground: 15 / 21\n",
            "predicting for Ground: 16 / 21\n",
            "predicting for Ground: 17 / 21\n",
            "predicting for Ground: 18 / 21\n",
            "predicting for Ground: 19 / 21\n",
            "predicting for Ground: 20 / 21\n",
            "predicting for Ground: 21 / 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvePbG5aQ30-",
        "outputId": "0ba0799f-d984-477d-df7b-3036be4f3edd"
      },
      "source": [
        "!zip -r BDRAR_test.zip Output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: Output/ (stored 0%)\n",
            "  adding: Output/Output20.jpg (deflated 13%)\n",
            "  adding: Output/Output8.jpg (deflated 2%)\n",
            "  adding: Output/Output5.jpg (deflated 30%)\n",
            "  adding: Output/Output14.jpg (deflated 36%)\n",
            "  adding: Output/Output3.jpg (deflated 10%)\n",
            "  adding: Output/Output4.jpg (deflated 51%)\n",
            "  adding: Output/Output19.jpg (deflated 24%)\n",
            "  adding: Output/Output9.jpg (deflated 4%)\n",
            "  adding: Output/Output2.jpg (deflated 39%)\n",
            "  adding: Output/Output6.jpg (deflated 13%)\n",
            "  adding: Output/Output12.jpg (deflated 20%)\n",
            "  adding: Output/Output15.jpg (deflated 11%)\n",
            "  adding: Output/Output11.jpg (deflated 36%)\n",
            "  adding: Output/Output16.jpg (deflated 18%)\n",
            "  adding: Output/Output10.jpg (deflated 67%)\n",
            "  adding: Output/Output7.jpg (deflated 36%)\n",
            "  adding: Output/Output1.jpg (deflated 43%)\n",
            "  adding: Output/Output18.jpg (deflated 16%)\n",
            "  adding: Output/Output17.jpg (deflated 37%)\n",
            "  adding: Output/Output13.jpg (deflated 48%)\n",
            "  adding: Output/Output0.jpg (deflated 36%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Uw5TIOEZwa",
        "outputId": "8d5df907-603c-4c3f-d9f6-2fb3f16f6d7c"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('./BDRAR_test.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_da0d0791-2a69-4406-9d4f-be4a2b57b071\", \"BDRAR_test.zip\", 635629)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx7FOqMz9XHv"
      },
      "source": [
        "if os.path.exists('./ckpt'):\n",
        "  !cp -r ./ckpt \"/content/gdrive/My Drive/BDRAR\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUraegFNLh8-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}